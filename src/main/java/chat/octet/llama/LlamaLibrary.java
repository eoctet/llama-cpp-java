package chat.octet.llama;

import com.sun.jna.*;
import com.sun.jna.ptr.FloatByReference;
import com.sun.jna.ptr.IntByReference;
import com.sun.jna.ptr.PointerByReference;
import lombok.ToString;

import java.nio.ByteBuffer;
import java.nio.FloatBuffer;
import java.nio.IntBuffer;
import java.util.Arrays;
import java.util.List;

/**
 * JNA Wrapper for library <b>libllama.dylib</b><br>
 * This file was autogenerated by <a href="http://jnaerator.googlecode.com/">JNAerator</a>,<br>
 * a tool written by <a href="http://ochafik.com/">Olivier Chafik</a> that <a href="http://code.google.com/p/jnaerator/wiki/CreditsAndLicense">uses a few opensource projects.</a>.<br>
 * For help, please visit <a href="http://nativelibs4java.googlecode.com/">NativeLibs4Java</a> , <a href="http://rococoa.dev.java.net/">Rococoa</a>, or <a href="http://jna.dev.java.net/">JNA</a>.
 */
public interface LlamaLibrary extends Library {
    String JNA_LIBRARY_NAME = "llama";
    NativeLibrary JNA_NATIVE_LIB = NativeLibrary.getInstance(LlamaLibrary.JNA_LIBRARY_NAME);
    LlamaLibrary INSTANCE = Native.load(LlamaLibrary.JNA_LIBRARY_NAME, LlamaLibrary.class);

    /**
     * enum values
     */
    interface llama_log_level {
        int LLAMA_LOG_LEVEL_ERROR = 2;
        int LLAMA_LOG_LEVEL_WARN = 3;
        int LLAMA_LOG_LEVEL_INFO = 4;
    }

    /**
     * enum values
     */
    interface llama_vocab_type {
        /**
         * SentencePiece
         */
        int LLAMA_VOCAB_TYPE_SPM = 0;
        /**
         * Byte Pair Encoding
         */
        int LLAMA_VOCAB_TYPE_BPE = 1;
    }

    /**
     * enum values
     */
    interface llama_token_type {
        int LLAMA_TOKEN_TYPE_UNDEFINED = 0;
        int LLAMA_TOKEN_TYPE_NORMAL = 1;
        int LLAMA_TOKEN_TYPE_UNKNOWN = 2;
        int LLAMA_TOKEN_TYPE_CONTROL = 3;
        int LLAMA_TOKEN_TYPE_USER_DEFINED = 4;
        int LLAMA_TOKEN_TYPE_UNUSED = 5;
        int LLAMA_TOKEN_TYPE_BYTE = 6;
    }

    /**
     * enum values
     */
    interface llama_ftype {
        int LLAMA_FTYPE_ALL_F32 = 0;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_F16 = 1;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q4_0 = 2;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q4_1 = 3;
        /**
         * tok_embeddings.weight and output.weight are F16
         */
        int LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q8_0 = 7;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q5_0 = 8;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q5_1 = 9;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q2_K = 10;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q3_K_S = 11;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q3_K_M = 12;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q3_K_L = 13;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q4_K_S = 14;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q4_K_M = 15;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q5_K_S = 16;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q5_K_M = 17;
        /**
         * except 1d tensors
         */
        int LLAMA_FTYPE_MOSTLY_Q6_K = 18;
        /**
         * not specified in the model file
         */
        int LLAMA_FTYPE_GUESSED = 1024;
    }

    /**
     * enum values
     */
    interface llama_gretype {
        int LLAMA_GRETYPE_END = 0;
        int LLAMA_GRETYPE_ALT = 1;
        int LLAMA_GRETYPE_RULE_REF = 2;
        int LLAMA_GRETYPE_CHAR = 3;
        int LLAMA_GRETYPE_CHAR_NOT = 4;
        int LLAMA_GRETYPE_CHAR_RNG_UPPER = 5;
        int LLAMA_GRETYPE_CHAR_ALT = 6;
    }

    int LLAMA_MAX_DEVICES = 1;
    long LLAMA_DEFAULT_SEED = 0xFFFFFFFFL;
    int LLAMA_FILE_MAGIC_GGSN = 0x6767736e;
    int LLAMA_SESSION_MAGIC = 0x6767736e;
    int LLAMA_SESSION_VERSION = 1;

    @ToString
    class llama_token_data extends Structure {
        /**
         * token id<br>
         * C type : llama_token
         */
        public int id;
        /**
         * log-odds of the token
         */
        public float logit;
        /**
         * probability of the token
         */
        public float p;

        public llama_token_data() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("id", "logit", "p");
        }

        /**
         * @param id    token id<br>
         *              C type : llama_token<br>
         * @param logit log-odds of the token<br>
         * @param p     probability of the token
         */
        public llama_token_data(int id, float logit, float p) {
            super();
            this.id = id;
            this.logit = logit;
            this.p = p;
        }

        public llama_token_data(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_token_data implements Structure.ByReference {

        }

        public static class ByValue extends llama_token_data implements Structure.ByValue {

        }
    }

    @ToString
    class llama_token_data_array extends Structure {
        /**
         * C type : llama_token_data*
         */
        public llama_token_data.ByReference data;
        public NativeSize size;
        public byte sorted;

        public llama_token_data_array() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("data", "size", "sorted");
        }

        /**
         * @param data C type : llama_token_data*
         */
        public llama_token_data_array(llama_token_data.ByReference data, NativeSize size, byte sorted) {
            super();
            this.data = data;
            this.size = size;
            this.sorted = sorted;
        }

        public llama_token_data_array(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_token_data_array implements Structure.ByReference {

        }

        public static class ByValue extends llama_token_data_array implements Structure.ByValue {

        }
    }

    @ToString
    class llama_context_params extends Structure {
        /**
         * RNG seed, -1 for random
         */
        public int seed;
        /**
         * text context
         */
        public int n_ctx;
        /**
         * prompt processing batch size
         */
        public int n_batch;
        /**
         * number of layers to store in VRAM
         */
        public int n_gpu_layers;
        /**
         * the GPU that is used for scratch and small tensors
         */
        public int main_gpu;
        /**
         * how to split layers across multiple GPUs (size: LLAMA_MAX_DEVICES)<br>
         * C type : const float*
         */
        public FloatByReference tensor_split;
        /**
         * RoPE base frequency
         */
        public float rope_freq_base;
        /**
         * RoPE frequency scaling factor
         */
        public float rope_freq_scale;
        /**
         * C type : llama_progress_callback
         */
        public llama_progress_callback progress_callback;
        /**
         * C type : void*
         */
        public Pointer progress_callback_user_data;
        /**
         * if true, reduce VRAM usage at the cost of performance
         */
        public byte low_vram;
        /**
         * if true, use experimental mul_mat_q kernels
         */
        public byte mul_mat_q;
        /**
         * use fp16 for KV cache
         */
        public byte f16_kv;
        /**
         * the llama_eval() call computes all logits, not just the last one
         */
        public byte logits_all;
        /**
         * only load the vocabulary, no weights
         */
        public byte vocab_only;
        /**
         * use mmap if possible
         */
        public byte use_mmap;
        /**
         * force system to keep model in RAM
         */
        public byte use_mlock;
        /**
         * embedding mode only
         */
        public byte embedding;

        public llama_context_params() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("seed", "n_ctx", "n_batch", "n_gpu_layers", "main_gpu", "tensor_split", "rope_freq_base", "rope_freq_scale", "progress_callback", "progress_callback_user_data", "low_vram", "mul_mat_q", "f16_kv", "logits_all", "vocab_only", "use_mmap", "use_mlock", "embedding");
        }

        public llama_context_params(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_context_params implements Structure.ByReference {

        }

        public static class ByValue extends llama_context_params implements Structure.ByValue {

        }
    }

    @ToString
    class llama_model_quantize_params extends Structure {
        /**
         * number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
         */
        public int nthread;
        /**
         * @see llama_ftype
         * quantize to this llama_ftype<br>
         * C type : llama_ftype
         */
        public int ftype;
        /**
         * allow quantizing non-f32/f16 tensors
         */
        public byte allow_requantize;
        /**
         * quantize output.weight
         */
        public byte quantize_output_tensor;

        public llama_model_quantize_params() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("nthread", "ftype", "allow_requantize", "quantize_output_tensor");
        }

        /**
         * @param nthread                number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()<br>
         * @param ftype                  @see llama_ftype<br>
         *                               quantize to this llama_ftype<br>
         *                               C type : llama_ftype<br>
         * @param allow_requantize       allow quantizing non-f32/f16 tensors<br>
         * @param quantize_output_tensor quantize output.weight
         */
        public llama_model_quantize_params(int nthread, int ftype, byte allow_requantize, byte quantize_output_tensor) {
            super();
            this.nthread = nthread;
            this.ftype = ftype;
            this.allow_requantize = allow_requantize;
            this.quantize_output_tensor = quantize_output_tensor;
        }

        public llama_model_quantize_params(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_model_quantize_params implements Structure.ByReference {

        }

        public static class ByValue extends llama_model_quantize_params implements Structure.ByValue {

        }
    }

    @ToString
    class llama_grammar_element extends Structure {
        /**
         * @see llama_gretype
         * C type : llama_gretype
         */
        public int type;
        /**
         * Unicode code point or rule ID
         */
        public int value;

        public llama_grammar_element() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("type", "value");
        }

        /**
         * @param type  @see llama_gretype<br>
         *              C type : llama_gretype<br>
         * @param value Unicode code point or rule ID
         */
        public llama_grammar_element(int type, int value) {
            super();
            this.type = type;
            this.value = value;
        }

        public llama_grammar_element(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_grammar_element implements Structure.ByReference {

        }

        public static class ByValue extends llama_grammar_element implements Structure.ByValue {

        }
    }

    @ToString
    class llama_timings extends Structure {
        public double t_start_ms;
        public double t_end_ms;
        public double t_load_ms;
        public double t_sample_ms;
        public double t_p_eval_ms;
        public double t_eval_ms;
        public int n_sample;
        public int n_p_eval;
        public int n_eval;

        public llama_timings() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("t_start_ms", "t_end_ms", "t_load_ms", "t_sample_ms", "t_p_eval_ms", "t_eval_ms", "n_sample", "n_p_eval", "n_eval");
        }

        public llama_timings(double t_start_ms, double t_end_ms, double t_load_ms, double t_sample_ms, double t_p_eval_ms, double t_eval_ms, int n_sample, int n_p_eval, int n_eval) {
            super();
            this.t_start_ms = t_start_ms;
            this.t_end_ms = t_end_ms;
            this.t_load_ms = t_load_ms;
            this.t_sample_ms = t_sample_ms;
            this.t_p_eval_ms = t_p_eval_ms;
            this.t_eval_ms = t_eval_ms;
            this.n_sample = n_sample;
            this.n_p_eval = n_p_eval;
            this.n_eval = n_eval;
        }

        public llama_timings(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_timings implements Structure.ByReference {

        }

        public static class ByValue extends llama_timings implements Structure.ByValue {

        }
    }

    @ToString
    class llama_beam_view extends Structure {
        /**
         * C type : const llama_token*
         */
        public IntByReference tokens;
        public NativeSize n_tokens;
        /**
         * Cumulative beam probability (renormalized relative to all beams)
         */
        public float p;
        /**
         * Callback should set this to true when a beam is at end-of-beam.
         */
        public byte eob;

        public llama_beam_view() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("tokens", "n_tokens", "p", "eob");
        }

        /**
         * @param tokens C type : const llama_token*<br>
         * @param p      Cumulative beam probability (renormalized relative to all beams)<br>
         * @param eob    Callback should set this to true when a beam is at end-of-beam.
         */
        public llama_beam_view(IntByReference tokens, NativeSize n_tokens, float p, byte eob) {
            super();
            this.tokens = tokens;
            this.n_tokens = n_tokens;
            this.p = p;
            this.eob = eob;
        }

        public llama_beam_view(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_beam_view implements Structure.ByReference {

        }

        public static class ByValue extends llama_beam_view implements Structure.ByValue {

        }
    }

    @ToString
    class llama_beams_state extends Structure {
        /**
         * C type : llama_beam_view*
         */
        public llama_beam_view.ByReference beam_views;
        /**
         * Number of elements in beam_views[].
         */
        public NativeSize n_beams;
        /**
         * Current max length of prefix tokens shared by all beams.
         */
        public NativeSize common_prefix_length;
        /**
         * True iff this is the last callback invocation.
         */
        public byte last_call;

        public llama_beams_state() {
            super();
        }

        protected List<String> getFieldOrder() {
            return Arrays.asList("beam_views", "n_beams", "common_prefix_length", "last_call");
        }

        /**
         * @param beam_views           C type : llama_beam_view*<br>
         * @param n_beams              Number of elements in beam_views[].<br>
         * @param common_prefix_length Current max length of prefix tokens shared by all beams.<br>
         * @param last_call            True iff this is the last callback invocation.
         */
        public llama_beams_state(llama_beam_view.ByReference beam_views, NativeSize n_beams, NativeSize common_prefix_length, byte last_call) {
            super();
            this.beam_views = beam_views;
            this.n_beams = n_beams;
            this.common_prefix_length = common_prefix_length;
            this.last_call = last_call;
        }

        public llama_beams_state(Pointer peer) {
            super(peer);
        }

        public static class ByReference extends llama_beams_state implements Structure.ByReference {

        }

        public static class ByValue extends llama_beams_state implements Structure.ByValue {

        }
    }

    interface llama_progress_callback extends Callback {
        void apply(float progress, Pointer ctx);
    }

    interface llama_log_callback extends Callback {
        void apply(int level, Pointer text, Pointer user_data);
    }

    interface llama_beam_search_callback_fn_t extends Callback {
        void apply(Pointer callback_data, llama_beams_state.ByValue llama_beams_state1);
    }

    /**
     * Original signature : <code>llama_context_params llama_context_default_params()</code>
     */
    llama_context_params.ByValue llama_context_default_params();

    /**
     * Original signature : <code>llama_model_quantize_params llama_model_quantize_default_params()</code>
     */
    llama_model_quantize_params.ByValue llama_model_quantize_default_params();

    /**
     * Original signature : <code>void llama_backend_init(bool)</code>
     */
    void llama_backend_init(byte numa);

    /**
     * Original signature : <code>void llama_backend_free()</code>
     */
    void llama_backend_free();

    /**
     * Original signature : <code>llama_model* llama_load_model_from_file(const char*, llama_context_params)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_load_model_from_file(String, llama_context_params.ByValue)} and {@link #llama_load_model_from_file(Pointer, llama_context_params.ByValue)} instead
     */
    @Deprecated
    llama_model llama_load_model_from_file(Pointer path_model, llama_context_params.ByValue params);

    /**
     * Original signature : <code>llama_model* llama_load_model_from_file(const char*, llama_context_params)</code>
     */
    llama_model llama_load_model_from_file(String path_model, llama_context_params.ByValue params);

    /**
     * Original signature : <code>void llama_free_model(llama_model*)</code>
     */
    void llama_free_model(llama_model model);

    /**
     * Original signature : <code>llama_context* llama_new_context_with_model(llama_model*, llama_context_params)</code>
     */
    llama_context llama_new_context_with_model(llama_model model, llama_context_params.ByValue params);

    /**
     * Original signature : <code>void llama_free(llama_context*)</code>
     */
    void llama_free(llama_context ctx);

    /**
     * Original signature : <code>int64_t llama_time_us()</code>
     */
    long llama_time_us();

    /**
     * Original signature : <code>int llama_max_devices()</code>
     */
    int llama_max_devices();

    /**
     * Original signature : <code>bool llama_mmap_supported()</code>
     */
    byte llama_mmap_supported();

    /**
     * Original signature : <code>bool llama_mlock_supported()</code>
     */
    byte llama_mlock_supported();

    /**
     * Original signature : <code>int llama_n_vocab(llama_context*)</code>
     */
    int llama_n_vocab(llama_context ctx);

    /**
     * Original signature : <code>int llama_n_ctx(llama_context*)</code>
     */
    int llama_n_ctx(llama_context ctx);

    /**
     * Original signature : <code>int llama_n_embd(llama_context*)</code>
     */
    int llama_n_embd(llama_context ctx);

    /**
     * Original signature : <code>llama_vocab_type llama_vocab_type(llama_context*)</code>
     */
    int llama_vocab_type(llama_context ctx);

    /**
     * Original signature : <code>int llama_model_n_vocab(llama_model*)</code>
     */
    int llama_model_n_vocab(llama_model model);

    /**
     * Original signature : <code>int llama_model_n_ctx(llama_model*)</code>
     */
    int llama_model_n_ctx(llama_model model);

    /**
     * Original signature : <code>int llama_model_n_embd(llama_model*)</code>
     */
    int llama_model_n_embd(llama_model model);

    /**
     * Original signature : <code>int llama_model_desc(llama_model*, char*, size_t)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_model_desc(llama_model, String, com.ochafik.lang.jnaerator.runtime.NativeSize)} and {@link #llama_model_desc(llama_model, Pointer, com.ochafik.lang.jnaerator.runtime.NativeSize)} instead
     */
    @Deprecated
    int llama_model_desc(llama_model model, Pointer buf, NativeSize buf_size);

    /**
     * Original signature : <code>int llama_model_desc(llama_model*, char*, size_t)</code>
     */
    int llama_model_desc(llama_model model, String buf, NativeSize buf_size);

    /**
     * Original signature : <code>uint64_t llama_model_size(llama_model*)</code>
     */
    long llama_model_size(llama_model model);

    /**
     * Original signature : <code>uint64_t llama_model_n_params(llama_model*)</code>
     */
    long llama_model_n_params(llama_model model);

    /**
     * Original signature : <code>int llama_model_quantize(const char*, const char*, const llama_model_quantize_params*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_model_quantize(String, String, llama_model_quantize_params)} and {@link #llama_model_quantize(Pointer, Pointer, llama_model_quantize_params)} instead
     */
    @Deprecated
    int llama_model_quantize(Pointer fname_inp, Pointer fname_out, llama_model_quantize_params params);

    /**
     * Original signature : <code>int llama_model_quantize(const char*, const char*, const llama_model_quantize_params*)</code>
     */
    int llama_model_quantize(String fname_inp, String fname_out, llama_model_quantize_params params);

    /**
     * Original signature : <code>int llama_apply_lora_from_file(llama_context*, const char*, const char*, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_apply_lora_from_file(llama_context, String, String, int)} and {@link #llama_apply_lora_from_file(llama_context, Pointer, Pointer, int)} instead
     */
    @Deprecated
    int llama_apply_lora_from_file(llama_context ctx, Pointer path_lora, Pointer path_base_model, int n_threads);

    /**
     * Original signature : <code>int llama_apply_lora_from_file(llama_context*, const char*, const char*, int)</code>
     */
    int llama_apply_lora_from_file(llama_context ctx, String path_lora, String path_base_model, int n_threads);

    /**
     * Original signature : <code>int llama_model_apply_lora_from_file(llama_model*, const char*, const char*, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_model_apply_lora_from_file(llama_model, String, String, int)} and {@link #llama_model_apply_lora_from_file(llama_model, Pointer, Pointer, int)} instead
     */
    @Deprecated
    int llama_model_apply_lora_from_file(llama_model model, Pointer path_lora, Pointer path_base_model, int n_threads);

    /**
     * Original signature : <code>int llama_model_apply_lora_from_file(llama_model*, const char*, const char*, int)</code>
     */
    int llama_model_apply_lora_from_file(llama_model model, String path_lora, String path_base_model, int n_threads);

    /**
     * Original signature : <code>int llama_get_kv_cache_token_count(llama_context*)</code>
     */
    int llama_get_kv_cache_token_count(llama_context ctx);

    /**
     * Original signature : <code>void llama_set_rng_seed(llama_context*, uint32_t)</code>
     */
    void llama_set_rng_seed(llama_context ctx, int seed);

    /**
     * Original signature : <code>size_t llama_get_state_size(llama_context*)</code>
     */
    NativeSize llama_get_state_size(llama_context ctx);

    /**
     * Original signature : <code>size_t llama_copy_state_data(llama_context*, uint8_t*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_copy_state_data(llama_context, ByteBuffer)} and {@link #llama_copy_state_data(llama_context, Pointer)} instead
     */
    @Deprecated
    NativeSize llama_copy_state_data(llama_context ctx, Pointer dst);

    /**
     * Original signature : <code>size_t llama_copy_state_data(llama_context*, uint8_t*)</code>
     */
    NativeSize llama_copy_state_data(llama_context ctx, ByteBuffer dst);

    /**
     * Original signature : <code>size_t llama_set_state_data(llama_context*, uint8_t*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_set_state_data(llama_context, ByteBuffer)} and {@link #llama_set_state_data(llama_context, Pointer)} instead
     */
    @Deprecated
    NativeSize llama_set_state_data(llama_context ctx, Pointer src);

    /**
     * Original signature : <code>size_t llama_set_state_data(llama_context*, uint8_t*)</code>
     */
    NativeSize llama_set_state_data(llama_context ctx, ByteBuffer src);

    /**
     * Original signature : <code>bool llama_load_session_file(llama_context*, const char*, llama_token*, size_t, size_t*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_load_session_file(llama_context, String, IntBuffer, com.ochafik.lang.jnaerator.runtime.NativeSize, com.ochafik.lang.jnaerator.runtime.NativeSizeByReference)} and {@link #llama_load_session_file(llama_context, Pointer, IntByReference, com.ochafik.lang.jnaerator.runtime.NativeSize, com.ochafik.lang.jnaerator.runtime.NativeSizeByReference)} instead
     */
    @Deprecated
    byte llama_load_session_file(llama_context ctx, Pointer path_session, IntByReference tokens_out, NativeSize n_token_capacity, NativeSizeByReference n_token_count_out);

    /**
     * Original signature : <code>bool llama_load_session_file(llama_context*, const char*, llama_token*, size_t, size_t*)</code>
     */
    byte llama_load_session_file(llama_context ctx, String path_session, IntBuffer tokens_out, NativeSize n_token_capacity, NativeSizeByReference n_token_count_out);

    /**
     * Original signature : <code>bool llama_save_session_file(llama_context*, const char*, const llama_token*, size_t)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_save_session_file(llama_context, String, IntBuffer, com.ochafik.lang.jnaerator.runtime.NativeSize)} and {@link #llama_save_session_file(llama_context, Pointer, IntByReference, com.ochafik.lang.jnaerator.runtime.NativeSize)} instead
     */
    @Deprecated
    byte llama_save_session_file(llama_context ctx, Pointer path_session, IntByReference tokens, NativeSize n_token_count);

    /**
     * Original signature : <code>bool llama_save_session_file(llama_context*, const char*, const llama_token*, size_t)</code>
     */
    byte llama_save_session_file(llama_context ctx, String path_session, IntBuffer tokens, NativeSize n_token_count);

    /**
     * Original signature : <code>int llama_eval(llama_context*, const llama_token*, int, int, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_eval(llama_context, IntBuffer, int, int, int)} and {@link #llama_eval(llama_context, IntByReference, int, int, int)} instead
     */
    @Deprecated
    int llama_eval(llama_context ctx, IntByReference tokens, int n_tokens, int n_past, int n_threads);

    /**
     * Original signature : <code>int llama_eval(llama_context*, const llama_token*, int, int, int)</code>
     */
    int llama_eval(llama_context ctx, IntBuffer tokens, int n_tokens, int n_past, int n_threads);

    /**
     * Original signature : <code>int llama_eval_embd(llama_context*, const float*, int, int, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_eval_embd(llama_context, float[], int, int, int)} and {@link #llama_eval_embd(llama_context, FloatByReference, int, int, int)} instead
     */
    @Deprecated
    int llama_eval_embd(llama_context ctx, FloatByReference embd, int n_tokens, int n_past, int n_threads);

    /**
     * Original signature : <code>int llama_eval_embd(llama_context*, const float*, int, int, int)</code>
     */
    int llama_eval_embd(llama_context ctx, float embd[], int n_tokens, int n_past, int n_threads);

    /**
     * Original signature : <code>int llama_eval_export(llama_context*, const char*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_eval_export(llama_context, String)} and {@link #llama_eval_export(llama_context, Pointer)} instead
     */
    @Deprecated
    int llama_eval_export(llama_context ctx, Pointer fname);

    /**
     * Original signature : <code>int llama_eval_export(llama_context*, const char*)</code>
     */
    int llama_eval_export(llama_context ctx, String fname);

    /**
     * Original signature : <code>float* llama_get_logits(llama_context*)</code>
     */
    FloatByReference llama_get_logits(llama_context ctx);

    /**
     * Original signature : <code>float* llama_get_embeddings(llama_context*)</code>
     */
    FloatByReference llama_get_embeddings(llama_context ctx);

    /**
     * Original signature : <code>char* llama_token_get_text(llama_context*, llama_token)</code>
     */
    String llama_token_get_text(llama_context ctx, int token);

    /**
     * Original signature : <code>float llama_token_get_score(llama_context*, llama_token)</code>
     */
    float llama_token_get_score(llama_context ctx, int token);

    /**
     * Original signature : <code>llama_token_type llama_token_get_type(llama_context*, llama_token)</code>
     */
    int llama_token_get_type(llama_context ctx, int token);

    /**
     * beginning-of-sentence<br>
     * Original signature : <code>llama_token llama_token_bos(llama_context*)</code>
     */
    int llama_token_bos(llama_context ctx);

    /**
     * end-of-sentence<br>
     * Original signature : <code>llama_token llama_token_eos(llama_context*)</code>
     */
    int llama_token_eos(llama_context ctx);

    /**
     * next-line<br>
     * Original signature : <code>llama_token llama_token_nl(llama_context*)</code>
     */
    int llama_token_nl(llama_context ctx);

    /**
     * Original signature : <code>int llama_tokenize(llama_context*, const char*, llama_token*, int, bool)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_tokenize(llama_context, String, IntBuffer, int, byte)} and {@link #llama_tokenize(llama_context, Pointer, IntByReference, int, byte)} instead
     */
    @Deprecated
    int llama_tokenize(llama_context ctx, Pointer text, IntByReference tokens, int n_max_tokens, byte add_bos);

    /**
     * Original signature : <code>int llama_tokenize(llama_context*, const char*, llama_token*, int, bool)</code>
     */
    int llama_tokenize(llama_context ctx, String text, IntBuffer tokens, int n_max_tokens, byte add_bos);

    /**
     * Original signature : <code>int llama_tokenize_with_model(llama_model*, const char*, llama_token*, int, bool)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_tokenize_with_model(llama_model, String, IntBuffer, int, byte)} and {@link #llama_tokenize_with_model(llama_model, Pointer, IntByReference, int, byte)} instead
     */
    @Deprecated
    int llama_tokenize_with_model(llama_model model, Pointer text, IntByReference tokens, int n_max_tokens, byte add_bos);

    /**
     * Original signature : <code>int llama_tokenize_with_model(llama_model*, const char*, llama_token*, int, bool)</code>
     */
    int llama_tokenize_with_model(llama_model model, String text, IntBuffer tokens, int n_max_tokens, byte add_bos);

    /**
     * Original signature : <code>int llama_token_to_piece(llama_context*, llama_token, char*, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_token_to_piece(llama_context, int, byte[], int)} and {@link #llama_token_to_piece(llama_context, int, Pointer, int)} instead
     */
    @Deprecated
    int llama_token_to_piece(llama_context ctx, int token, Pointer buf, int length);

    /**
     * Original signature : <code>int llama_token_to_piece(llama_context*, llama_token, char*, int)</code>
     */
    int llama_token_to_piece(llama_context ctx, int token, byte[] buf, int length);

    /**
     * Original signature : <code>int llama_token_to_piece_with_model(llama_model*, llama_token, char*, int)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_token_to_piece_with_model(llama_model, int, byte[], int)} and {@link #llama_token_to_piece_with_model(llama_model, int, Pointer, int)} instead
     */
    @Deprecated
    int llama_token_to_piece_with_model(llama_model model, int token, Pointer buf, int length);

    /**
     * Original signature : <code>int llama_token_to_piece_with_model(llama_model*, llama_token, char*, int)</code>
     */
    int llama_token_to_piece_with_model(llama_model model, int token, byte[] buf, int length);

    /**
     * Original signature : <code>llama_grammar* llama_grammar_init(const llama_grammar_element**, size_t, size_t)</code><br>
     *
     * @deprecated use the safer method {@link #llama_grammar_init(llama_grammar_element.ByReference[], com.ochafik.lang.jnaerator.runtime.NativeSize, com.ochafik.lang.jnaerator.runtime.NativeSize)} instead
     */
    @Deprecated
    llama_grammar llama_grammar_init(PointerByReference rules, NativeSize n_rules, NativeSize start_rule_index);

    /**
     * Original signature : <code>llama_grammar* llama_grammar_init(const llama_grammar_element**, size_t, size_t)</code>
     */
    llama_grammar llama_grammar_init(llama_grammar_element.ByReference rules[], NativeSize n_rules, NativeSize start_rule_index);

    /**
     * Original signature : <code>void llama_grammar_free(llama_grammar*)</code>
     */
    void llama_grammar_free(llama_grammar grammar);

    /**
     * Original signature : <code>void llama_sample_repetition_penalty(llama_context*, llama_token_data_array*, const llama_token*, size_t, float)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_sample_repetition_penalty(llama_context, llama_token_data_array, IntBuffer, com.ochafik.lang.jnaerator.runtime.NativeSize, float)} and {@link #llama_sample_repetition_penalty(llama_context, llama_token_data_array, IntByReference, com.ochafik.lang.jnaerator.runtime.NativeSize, float)} instead
     */
    @Deprecated
    void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, IntByReference last_tokens, NativeSize last_tokens_size, float penalty);

    /**
     * Original signature : <code>void llama_sample_repetition_penalty(llama_context*, llama_token_data_array*, const llama_token*, size_t, float)</code>
     */
    void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, IntBuffer last_tokens, NativeSize last_tokens_size, float penalty);

    /**
     * Original signature : <code>void llama_sample_frequency_and_presence_penalties(llama_context*, llama_token_data_array*, const llama_token*, size_t, float, float)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_sample_frequency_and_presence_penalties(llama_context, llama_token_data_array, IntBuffer, com.ochafik.lang.jnaerator.runtime.NativeSize, float, float)} and {@link #llama_sample_frequency_and_presence_penalties(llama_context, llama_token_data_array, IntByReference, com.ochafik.lang.jnaerator.runtime.NativeSize, float, float)} instead
     */
    @Deprecated
    void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, IntByReference last_tokens, NativeSize last_tokens_size, float alpha_frequency, float alpha_presence);

    /**
     * Original signature : <code>void llama_sample_frequency_and_presence_penalties(llama_context*, llama_token_data_array*, const llama_token*, size_t, float, float)</code>
     */
    void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, IntBuffer last_tokens, NativeSize last_tokens_size, float alpha_frequency, float alpha_presence);

    /**
     * Original signature : <code>void llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float)</code>
     */
    void llama_sample_classifier_free_guidance(llama_context ctx, llama_token_data_array candidates, llama_context guidance_ctx, float scale);

    /**
     * Original signature : <code>void llama_sample_softmax(llama_context*, llama_token_data_array*)</code>
     */
    void llama_sample_softmax(llama_context ctx, llama_token_data_array candidates);

    /**
     * Original signature : <code>void llama_sample_top_k(llama_context*, llama_token_data_array*, int, size_t)</code>
     */
    void llama_sample_top_k(llama_context ctx, llama_token_data_array candidates, int k, NativeSize min_keep);

    /**
     * Original signature : <code>void llama_sample_top_p(llama_context*, llama_token_data_array*, float, size_t)</code>
     */
    void llama_sample_top_p(llama_context ctx, llama_token_data_array candidates, float p, NativeSize min_keep);

    /**
     * Original signature : <code>void llama_sample_tail_free(llama_context*, llama_token_data_array*, float, size_t)</code>
     */
    void llama_sample_tail_free(llama_context ctx, llama_token_data_array candidates, float z, NativeSize min_keep);

    /**
     * Original signature : <code>void llama_sample_typical(llama_context*, llama_token_data_array*, float, size_t)</code>
     */
    void llama_sample_typical(llama_context ctx, llama_token_data_array candidates, float p, NativeSize min_keep);

    /**
     * Original signature : <code>void llama_sample_temperature(llama_context*, llama_token_data_array*, float)</code>
     */
    void llama_sample_temperature(llama_context ctx, llama_token_data_array candidates, float temp);

    /**
     * Original signature : <code>void llama_sample_grammar(llama_context*, llama_token_data_array*, llama_grammar*)</code>
     */
    void llama_sample_grammar(llama_context ctx, llama_token_data_array candidates, llama_grammar grammar);

    /**
     * Original signature : <code>llama_token llama_sample_token_mirostat(llama_context*, llama_token_data_array*, float, float, int, float*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_sample_token_mirostat(llama_context, llama_token_data_array, float, float, int, FloatBuffer)} and {@link #llama_sample_token_mirostat(llama_context, llama_token_data_array, float, float, int, FloatByReference)} instead
     */
    @Deprecated
    int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatByReference mu);

    /**
     * Original signature : <code>llama_token llama_sample_token_mirostat(llama_context*, llama_token_data_array*, float, float, int, float*)</code>
     */
    int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatBuffer mu);

    /**
     * Original signature : <code>llama_token llama_sample_token_mirostat_v2(llama_context*, llama_token_data_array*, float, float, float*)</code><br>
     *
     * @deprecated use the safer methods {@link #llama_sample_token_mirostat_v2(llama_context, llama_token_data_array, float, float, FloatBuffer)} and {@link #llama_sample_token_mirostat_v2(llama_context, llama_token_data_array, float, float, FloatByReference)} instead
     */
    @Deprecated
    int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatByReference mu);

    /**
     * Original signature : <code>llama_token llama_sample_token_mirostat_v2(llama_context*, llama_token_data_array*, float, float, float*)</code>
     */
    int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatBuffer mu);

    /**
     * Original signature : <code>llama_token llama_sample_token_greedy(llama_context*, llama_token_data_array*)</code>
     */
    int llama_sample_token_greedy(llama_context ctx, llama_token_data_array candidates);

    /**
     * Original signature : <code>llama_token llama_sample_token(llama_context*, llama_token_data_array*)</code>
     */
    int llama_sample_token(llama_context ctx, llama_token_data_array candidates);

    /**
     * Original signature : <code>void llama_grammar_accept_token(llama_context*, llama_grammar*, llama_token)</code>
     */
    void llama_grammar_accept_token(llama_context ctx, llama_grammar grammar, int token);

    /**
     * Original signature : <code>void llama_beam_search(llama_context*, llama_beam_search_callback_fn_t, void*, size_t, int, int, int)</code>
     */
    void llama_beam_search(llama_context ctx, llama_beam_search_callback_fn_t callback, Pointer callback_data, NativeSize n_beams, int n_past, int n_predict, int n_threads);

    /**
     * Original signature : <code>llama_timings llama_get_timings(llama_context*)</code>
     */
    llama_timings.ByValue llama_get_timings(llama_context ctx);

    /**
     * Original signature : <code>void llama_print_timings(llama_context*)</code>
     */
    void llama_print_timings(llama_context ctx);

    /**
     * Original signature : <code>void llama_reset_timings(llama_context*)</code>
     */
    void llama_reset_timings(llama_context ctx);

    /**
     * Original signature : <code>char* llama_print_system_info()</code>
     */
    String llama_print_system_info();

    /**
     * Original signature : <code>void llama_log_set(llama_log_callback, void*)</code>
     */
    void llama_log_set(llama_log_callback log_callback, Pointer user_data);

    /**
     * Original signature : <code>void llama_dump_timing_info_yaml(FILE*, llama_context*)</code>
     */
    void llama_dump_timing_info_yaml(FILE stream, llama_context ctx);

    class llama_grammar extends PointerType {
        public llama_grammar(Pointer address) {
            super(address);
        }

        public llama_grammar() {
            super();
        }
    }

    class FILE extends PointerType {
        public FILE(Pointer address) {
            super(address);
        }

        public FILE() {
            super();
        }
    }

    class llama_model extends PointerType {
        public llama_model(Pointer address) {
            super(address);
        }

        public llama_model() {
            super();
        }
    }

    class llama_context extends PointerType {
        public llama_context(Pointer address) {
            super(address);
        }

        public llama_context() {
            super();
        }
    }
}
